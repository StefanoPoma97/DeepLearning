{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "autograd.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ctFkKBDPUyj",
        "colab_type": "text"
      },
      "source": [
        "# Automatic Differentiation\n",
        "\n",
        "## Import `autograd` and create a variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAcLb5cLPUyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "1"
        },
        "collapsed": true,
        "id": "VPFQpHKnPUyy",
        "colab_type": "code",
        "colab": {},
        "outputId": "3fd56d00-de9b-437a-b7be-2ef850d45f1c"
      },
      "source": [
        "from mxnet import autograd, nd\n",
        "\n",
        "x = nd.arange(4).reshape((4, 1))\n",
        "print(x) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[[0.]\n",
            " [1.]\n",
            " [2.]\n",
            " [3.]]\n",
            "<NDArray 4x1 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppmQOf5MPUy7",
        "colab_type": "text"
      },
      "source": [
        "## Attach gradient to `x`\n",
        "\n",
        "- It allocates memory to store its gradient, which has the same shape as `x`. \n",
        "- It also tell the system that we need to compute its gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "3"
        },
        "collapsed": true,
        "id": "acUofzboPUy-",
        "colab_type": "code",
        "colab": {},
        "outputId": "022c3168-fadf-4d2e-f3f1-6905dc4a36c7"
      },
      "source": [
        "x.attach_grad()\n",
        "x.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[0.]\n",
              " [0.]\n",
              " [0.]\n",
              " [0.]]\n",
              "<NDArray 4x1 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkxVFd3sPUzF",
        "colab_type": "text"
      },
      "source": [
        "## Forward\n",
        "\n",
        "Now compute \n",
        "\n",
        "$$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$$\n",
        "\n",
        "by placing code inside a ``with autograd.record():`` block. MXNet will build the according computation graph. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "4"
        },
        "collapsed": true,
        "id": "JGdamZVAPUzK",
        "colab_type": "code",
        "colab": {},
        "outputId": "1336c106-2b45-413d-adae-612bebaac538"
      },
      "source": [
        "with autograd.record():\n",
        "    y = 2 * nd.dot(x.T, x)\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[28.]]\n",
              "<NDArray 1x1 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqvoN5ucPUzR",
        "colab_type": "text"
      },
      "source": [
        "## Backward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "5"
        },
        "id": "dplz41DGPUzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXWIkfZZPUzc",
        "colab_type": "text"
      },
      "source": [
        "## Get the gradient\n",
        "\n",
        "Given $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$, we know \n",
        "\n",
        "$$\\frac{\\partial y}{\\partial \\mathbf x} = 4\\mathbf{x}$$\n",
        "\n",
        "Now verify the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "6"
        },
        "collapsed": true,
        "id": "Z91_4aSVPUzf",
        "colab_type": "code",
        "colab": {},
        "outputId": "cd1771ff-d257-4e8f-a5be-3e8cc5dd2ab7"
      },
      "source": [
        "print((x.grad - 4 * x).norm().asscalar() == 0)\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "\n",
            "[[ 0.]\n",
            " [ 4.]\n",
            " [ 8.]\n",
            " [12.]]\n",
            "<NDArray 4x1 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_VekCK1PUzn",
        "colab_type": "text"
      },
      "source": [
        "## Backward on non-scalar\n",
        "\n",
        "`y.backward()` equals to `y.sum().backward()`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vkxW7dbyPUzp",
        "colab_type": "code",
        "colab": {},
        "outputId": "9919bf8b-1951-4faf-d724-3121db8d750e"
      },
      "source": [
        "with autograd.record():\n",
        "    y = 2 * x * x\n",
        "print(y.shape)\n",
        "y.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 1)\n",
            "\n",
            "[[ 0.]\n",
            " [ 4.]\n",
            " [ 8.]\n",
            " [12.]]\n",
            "<NDArray 4x1 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq0X28VaPUzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPSSrMoqPUzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xpI-JgyPUz4",
        "colab_type": "text"
      },
      "source": [
        "## Training mode and prediction mode\n",
        "\n",
        "The `record` scope will alter the mode by assuming that gradient is only required for training. \n",
        "\n",
        "It's necessary since some layers, e.g. batch normalization, behavior differently in the training and prediction modes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "7"
        },
        "collapsed": true,
        "id": "Yz4EWiT-PUz5",
        "colab_type": "code",
        "colab": {},
        "outputId": "7b4bf5b9-938d-4254-b65c-192970c93a0b"
      },
      "source": [
        "print(autograd.is_training())\n",
        "with autograd.record():\n",
        "    print(autograd.is_training())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jywy5EpPUz-",
        "colab_type": "text"
      },
      "source": [
        "## Computing the hradient of Python control flow\n",
        "\n",
        "Autograd also works with Python functions and control flows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "8"
        },
        "id": "biYYKJGBPUz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm().asscalar() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum().asscalar() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TqWge6qPU0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50szKY1cPU0I",
        "colab_type": "text"
      },
      "source": [
        "## Function behaviors depends on inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "id": "J3u5bwn_PU0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = nd.random.normal(shape=1)\n",
        "a.attach_grad()\n",
        "with autograd.record():\n",
        "    d = f(a)\n",
        "d.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAmnFlowPU0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYT5NQGlPU0S",
        "colab_type": "text"
      },
      "source": [
        "## Verify the results\n",
        "\n",
        "\n",
        "`f` is piecewise linear in its input `a`. There exists $g$ such as $f(a) = g a$ and $\\frac{\\partial f}{\\partial a}=g$. Verify the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "10"
        },
        "collapsed": true,
        "id": "mZH9ukXzPU0U",
        "colab_type": "code",
        "colab": {},
        "outputId": "261e505e-497e-4349-afe4-75d1b997ca1a"
      },
      "source": [
        "print(a.grad == (d / a))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[1.]\n",
            "<NDArray 1 @cpu(0)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC6-NyzxPU0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUnfjF1iPU0e",
        "colab_type": "text"
      },
      "source": [
        "## Head gradients and the chain rule\n",
        "\n",
        "We can break the chain rule manually. Assume $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial x}.$ `y.backward()` will only compute $\\frac{\\partial y}{\\partial x}$. To get $\\frac{\\partial z}{\\partial x}$, we can first compute $\\frac{\\partial z}{\\partial y}$, and then pass it as head gradient to `y.backward`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "collapsed": true,
        "id": "07abedNePU0f",
        "colab_type": "code",
        "colab": {},
        "outputId": "f5daf77a-8ad5-4ae2-ff7a-9eed99a64188"
      },
      "source": [
        "with autograd.record():\n",
        "    y = x * 2\n",
        "y.attach_grad()\n",
        "with autograd.record():\n",
        "    z = y * x\n",
        "z.backward()  # y.grad = \\partial z / \\partial y\n",
        "y.backward(y.grad)\n",
        "x.grad == 2*x # x.grad = \\partial z / \\partial x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "[[1.]\n",
              " [1.]\n",
              " [1.]\n",
              " [1.]]\n",
              "<NDArray 4x1 @cpu(0)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}