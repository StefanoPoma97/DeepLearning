{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "keras-06-visualizing-what-convnets-learn.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU82jEJYrom6",
        "colab_type": "code",
        "outputId": "795f7d98-de94-4303-953d-ae515fecb403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "keras.__version__\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTgbCCi6ZqIs",
        "colab_type": "code",
        "outputId": "561649ed-2581-4c4a-d0db-88e675531fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrMhH76RZqJF",
        "colab_type": "text"
      },
      "source": [
        "# Using convnets with small datasets\n",
        "\n",
        "This notebook contains the code sample found in Chapter 5, Section 2 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "## Training a convnet from scratch on a small dataset\n",
        "\n",
        "Having to train an image classification model using only very little data is a common situation, which you likely encounter yourself in \n",
        "practice if you ever do computer vision in a professional context.\n",
        "\n",
        "Having \"few\" samples can mean anywhere from a few hundreds to a few tens of thousands of images. As a practical example, we will focus on \n",
        "classifying images as \"dogs\" or \"cats\", in a dataset containing 4000 pictures of cats and dogs (2000 cats, 2000 dogs). We will use 2000 \n",
        "pictures for training, 1000 for validation, and finally 1000 for testing.\n",
        "\n",
        "In this section, we will review one basic strategy to tackle this problem: training a new model from scratch on what little data we have. We \n",
        "will start by naively training a small convnet on our 2000 training samples, without any regularization, to set a baseline for what can be \n",
        "achieved. This will get us to a classification accuracy of 71%. At that point, our main issue will be overfitting. Then we will introduce \n",
        "*data augmentation*, a powerful technique for mitigating overfitting in computer vision. By leveraging data augmentation, we will improve \n",
        "our network to reach an accuracy of 82%.\n",
        "\n",
        "In the next section, we will review two more essential techniques for applying deep learning to small datasets: *doing feature extraction \n",
        "with a pre-trained network* (this will get us to an accuracy of 90% to 93%), and *fine-tuning a pre-trained network* (this will get us to \n",
        "our final accuracy of 95%). Together, these three strategies -- training a small model from scratch, doing feature extracting using a \n",
        "pre-trained model, and fine-tuning a pre-trained model -- will constitute your future toolbox for tackling the problem of doing computer \n",
        "vision with small datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUGvsPd1ZqJL",
        "colab_type": "text"
      },
      "source": [
        "## The relevance of deep learning for small-data problems\n",
        "\n",
        "You will sometimes hear that deep learning only works when lots of data is available. This is in part a valid point: one fundamental \n",
        "characteristic of deep learning is that it is able to find interesting features in the training data on its own, without any need for manual \n",
        "feature engineering, and this can only be achieved when lots of training examples are available. This is especially true for problems where \n",
        "the input samples are very high-dimensional, like images.\n",
        "\n",
        "However, what constitutes \"lots\" of samples is relative -- relative to the size and depth of the network you are trying to train, for \n",
        "starters. It isn't possible to train a convnet to solve a complex problem with just a few tens of samples, but a few hundreds can \n",
        "potentially suffice if the model is small and well-regularized and if the task is simple. \n",
        "Because convnets learn local, translation-invariant features, they are very \n",
        "data-efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results \n",
        "despite a relative lack of data, without the need for any custom feature engineering. You will see this in action in this section.\n",
        "\n",
        "But what's more, deep learning models are by nature highly repurposable: you can take, say, an image classification or speech-to-text model \n",
        "trained on a large-scale dataset then reuse it on a significantly different problem with only minor changes. Specifically, in the case of \n",
        "computer vision, many pre-trained models (usually trained on the ImageNet dataset) are now publicly available for download and can be used \n",
        "to bootstrap powerful vision models out of very little data. That's what we will do in the next section.\n",
        "\n",
        "For now, let's get started by getting our hands on the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB0HKKGBZqJP",
        "colab_type": "text"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "The cats vs. dogs dataset that we will use isn't packaged with Keras. It was made available by Kaggle.com as part of a computer vision \n",
        "competition in late 2013, back when convnets weren't quite mainstream. You can download the original dataset at: \n",
        "`https://www.kaggle.com/c/dogs-vs-cats/data` (you will need to create a Kaggle account if you don't already have one -- don't worry, the \n",
        "process is painless).\n",
        "\n",
        "The pictures are medium-resolution color JPEGs. They look like this:\n",
        "\n",
        "![cats_vs_dogs_samples](https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I661xJfRZqJS",
        "colab_type": "text"
      },
      "source": [
        "Unsurprisingly, the cats vs. dogs Kaggle competition in 2013 was won by entrants who used convnets. The best entries could achieve up to \n",
        "95% accuracy. In our own example, we will get fairly close to this accuracy (in the next section), even though we will be training our \n",
        "models on less than 10% of the data that was available to the competitors.\n",
        "This original dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543MB large (compressed). After downloading \n",
        "and uncompressing it, we will create a new dataset containing three subsets: a training set with 1000 samples of each class, a validation \n",
        "set with 500 samples of each class, and finally a test set with 500 samples of each class.\n",
        "\n",
        "Here are a few lines of code to do this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYjBvPDg-qaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#taken from this StackOverflow answer: https://stackoverflow.com/a/39225039\n",
        "import requests, os, shutil\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9s6x7Pk-rUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_id = '1AmfeQmIF4wtQrQBXtISWNDAn7_K3Ojeb'\n",
        "destination = './cats_and_dogs_small.zip'\n",
        "download_file_from_google_drive(file_id, destination)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2b4r4gg_a7i",
        "colab_type": "code",
        "outputId": "51f3293b-7aa6-4b11-b0f0-c7834e4eb031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "os.system('unzip ./cats_and_dogs_small.zip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9IMaI36rpFOF",
        "colab": {}
      },
      "source": [
        "file_id = '17XxeFC5shh9CZKUWz6uJ8HS21u06v8MX'\n",
        "destination = './cats_and_dogs_small.h5'\n",
        "download_file_from_google_drive(file_id, destination)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VA5KyU0AU2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The path to the directory where the original\n",
        "# dataset was uncompressed\n",
        "original_dataset_dir = './cats_and_dogs_small'\n",
        "\n",
        "# The directory where we will\n",
        "# store our smaller dataset\n",
        "base_dir = './cats_and_dogs_small'\n",
        "#os.mkdir(base_dir)\n",
        "\n",
        "# Directories for our training,\n",
        "# validation and test splits\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "#os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "#os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "#os.mkdir(test_dir)\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "#os.mkdir(train_cats_dir)\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "#os.mkdir(train_dogs_dir)\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "#os.mkdir(validation_cats_dir)\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "#os.mkdir(validation_dogs_dir)\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "test_cats_dir = os.path.join(test_dir, 'cats')\n",
        "#os.mkdir(test_cats_dir)\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
        "#os.mkdir(test_dogs_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlX8MymWZqJs",
        "colab_type": "text"
      },
      "source": [
        "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olv3yjtwZqJ3",
        "colab_type": "code",
        "outputId": "b9374824-a59b-439e-9114-e34b05ab812a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('total training cat images:', len(os.listdir(train_cats_dir)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training cat images: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyaoZ2L2ZqKB",
        "colab_type": "code",
        "outputId": "dc3727be-21fe-4e1d-9342-e37a5f937590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total training dog images: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF6eSoEMZqKM",
        "colab_type": "code",
        "outputId": "b8fa231b-a99e-4da8-ae92-39770fe678a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total validation cat images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3ZqHALEZqKY",
        "colab_type": "code",
        "outputId": "f830c89d-f308-4445-caa0-6dd3dd7ea624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total validation dog images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTj1ljUCZqKh",
        "colab_type": "code",
        "outputId": "bcfe06ed-0aa8-4282-882c-a14c67e63707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('total test cat images:', len(os.listdir(test_cats_dir)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total test cat images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2eRJUTUZqKs",
        "colab_type": "code",
        "outputId": "a8561b9f-6458-4616-e7cd-0ff61b8ae65d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total test dog images: 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x378KBLZqK3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "So we have indeed 2000 training images, and then 1000 validation images and 1000 test images. In each split, there is the same number of \n",
        "samples from each class: this is a balanced binary classification problem, which means that classification accuracy will be an appropriate \n",
        "measure of success."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W_OGRiQrA9yG"
      },
      "source": [
        "# Visualizing what convnets learn\n",
        "\n",
        "This notebook contains the code sample found in Chapter 5, Section 4 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "It is often said that deep learning models are \"black boxes\", learning representations that are difficult to extract and present in a \n",
        "human-readable form. While this is partially true for certain types of deep learning models, it is definitely not true for convnets. The \n",
        "representations learned by convnets are highly amenable to visualization, in large part because they are _representations of visual \n",
        "concepts_. Since 2013, a wide array of techniques have been developed for visualizing and interpreting these representations. We won't \n",
        "survey all of them, but we will cover three of the most accessible and useful ones:\n",
        "\n",
        "* Visualizing intermediate convnet outputs (\"intermediate activations\"). This is useful to understand how successive convnet layers \n",
        "transform their input, and to get a first idea of the meaning of individual convnet filters.\n",
        "* Visualizing convnets filters. This is useful to understand precisely what visual pattern or concept each filter in a convnet is receptive \n",
        "to.\n",
        "* Visualizing heatmaps of class activation in an image. This is useful to understand which part of an image where identified as belonging \n",
        "to a given class, and thus allows to localize objects in images.\n",
        "\n",
        "For the first method -- activation visualization -- we will use the small convnet that we trained from scratch on the cat vs. dog \n",
        "classification problem two sections ago. For the next two methods, we will use the VGG16 model that we introduced in the previous section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hBNJkPlronY",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing intermediate activations\n",
        "\n",
        "Visualizing intermediate activations consists in displaying the feature maps that are output by various convolution and pooling layers in a \n",
        "network, given a certain input (the output of a layer is often called its \"activation\", the output of the activation function). This gives \n",
        "a view into how an input is decomposed unto the different filters learned by the network. These feature maps we want to visualize have 3 \n",
        "dimensions: width, height, and depth (channels). Each channel encodes relatively independent features, so the proper way to visualize these \n",
        "feature maps is by independently plotting the contents of every channel, as a 2D image.\n",
        "Let's start by loading the model that we saved in section 5.2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ixfotcjfronb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('cats_and_dogs_small.h5')\n",
        "model.summary()  # As a reminder."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KidzBo-bronn",
        "colab_type": "text"
      },
      "source": [
        "This will be the input image we will use -- a picture of a cat, not part of images that the network was trained on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UO_2zoironq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_path = './cats_and_dogs_small/test/cats/cat.1700.jpg'\n",
        "\n",
        "# We preprocess the image into a 4D tensor\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "# Remember that the model was trained on inputs\n",
        "# that were preprocessed in the following way:\n",
        "img_tensor /= 255.\n",
        "\n",
        "# Its shape is (1, 150, 150, 3)\n",
        "print(img_tensor.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6-2-JA9ronz",
        "colab_type": "text"
      },
      "source": [
        "Let's display our picture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibB1dUcJron2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNBVtKRDrooA",
        "colab_type": "text"
      },
      "source": [
        "In order to extract the feature maps we want to look at, we will create a Keras model that takes batches of images as input, and outputs \n",
        "the activations of all convolution and pooling layers. To do this, we will use the Keras class `Model`. A `Model` is instantiated using two \n",
        "arguments: an input tensor (or list of input tensors), and an output tensor (or list of output tensors). The resulting class is a Keras \n",
        "model, just like the `Sequential` models that you are familiar with, mapping the specified inputs to the specified outputs. What sets the \n",
        "`Model` class apart is that it allows for models with multiple outputs, unlike `Sequential`. For more information about the `Model` class, see \n",
        "Chapter 7, Section 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjSkrEILrooD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "\n",
        "# Extracts the outputs of the top 8 layers:\n",
        "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
        "# Creates a model that will return these outputs, given the model input:\n",
        "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihcrG0xDrooN",
        "colab_type": "text"
      },
      "source": [
        "When fed an image input, this model returns the values of the layer activations in the original model. This is the first time you encounter \n",
        "a multi-output model in this book: until now the models you have seen only had exactly one input and one output. In the general case, a \n",
        "model could have any number of inputs and outputs. This one has one input and 8 outputs, one output per layer activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXa5D_DIrooT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will return a list of 5 Numpy arrays:\n",
        "# one array per layer activation\n",
        "activations = activation_model.predict(img_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYkqUuznrood",
        "colab_type": "text"
      },
      "source": [
        "For instance, this is the activation of the first convolution layer for our cat image input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpMAfdvHroof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "first_layer_activation = activations[0]\n",
        "print(first_layer_activation.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43-PT2znroon",
        "colab_type": "text"
      },
      "source": [
        "It's a 148x148 feature map with 32 channels. Let's try visualizing the 3rd channel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SybITABhrooq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.matshow(first_layer_activation[0, :, :, 3], cmap='viridis')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrBjIIboroo0",
        "colab_type": "text"
      },
      "source": [
        "This channel appears to encode a diagonal edge detector. Let's try the 30th channel -- but note that your own channels may vary, since the \n",
        "specific filters learned by convolution layers are not deterministic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQKsH_Ehroo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0anOZmSroo_",
        "colab_type": "text"
      },
      "source": [
        "This one looks like a \"bright green dot\" detector, useful to encode cat eyes. At this point, let's go and plot a complete visualization of \n",
        "all the activations in the network. We'll extract and plot every channel in each of our 8 activation maps, and we will stack the results in \n",
        "one big image tensor, with channels stacked side by side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOKUSlZvropB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = []\n",
        "for layer in model.layers[:8]:\n",
        "    layer_names.append(layer.name)\n",
        "\n",
        "images_per_row = 16\n",
        "\n",
        "# Now let's display our feature maps\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    # This is the number of features in the feature map\n",
        "    n_features = layer_activation.shape[-1]\n",
        "\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = layer_activation.shape[1]\n",
        "\n",
        "    # We will tile the activation channels in this matrix\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "\n",
        "    # We'll tile each filter into this big horizontal grid\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0,\n",
        "                                             :, :,\n",
        "                                             col * images_per_row + row]\n",
        "            # Post-process the feature to make it visually palatable\n",
        "            channel_image -= channel_image.mean()\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "            display_grid[col * size : (col + 1) * size,\n",
        "                         row * size : (row + 1) * size] = channel_image\n",
        "\n",
        "    # Display the grid\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_wbdzoEropJ",
        "colab_type": "text"
      },
      "source": [
        "A few remarkable things to note here:\n",
        "\n",
        "* The first layer acts as a collection of various edge detectors. At that stage, the activations are still retaining almost all of the \n",
        "information present in the initial picture.\n",
        "* As we go higher-up, the activations become increasingly abstract and less visually interpretable. They start encoding higher-level \n",
        "concepts such as \"cat ear\" or \"cat eye\". Higher-up presentations carry increasingly less information about the visual contents of the \n",
        "image, and increasingly more information related to the class of the image.\n",
        "* The sparsity of the activations is increasing with the depth of the layer: in the first layer, all filters are activated by the input \n",
        "image, but in the following layers more and more filters are blank. This means that the pattern encoded by the filter isn't found in the \n",
        "input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaMVP7GsropM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We have just evidenced a very important universal characteristic of the representations learned by deep neural networks: the features \n",
        "extracted by a layer get increasingly abstract with the depth of the layer. The activations of layers higher-up carry less and less \n",
        "information about the specific input being seen, and more and more information about the target (in our case, the class of the image: cat \n",
        "or dog). A deep neural network effectively acts as an __information distillation pipeline__, with raw data going in (in our case, RBG \n",
        "pictures), and getting repeatedly transformed so that irrelevant information gets filtered out (e.g. the specific visual appearance of the \n",
        "image) while useful information get magnified and refined (e.g. the class of the image).\n",
        "\n",
        "This is analogous to the way humans and animals perceive the world: after observing a scene for a few seconds, a human can remember which \n",
        "abstract objects were present in it (e.g. bicycle, tree) but could not remember the specific appearance of these objects. In fact, if you \n",
        "tried to draw a generic bicycle from mind right now, chances are you could not get it even remotely right, even though you have seen \n",
        "thousands of bicycles in your lifetime. Try it right now: this effect is absolutely real. You brain has learned to completely abstract its \n",
        "visual input, to transform it into high-level visual concepts while completely filtering out irrelevant visual details, making it \n",
        "tremendously difficult to remember how things around us actually look."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dh-94boropP",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing convnet filters\n",
        "\n",
        "\n",
        "Another easy thing to do to inspect the filters learned by convnets is to display the visual pattern that each filter is meant to respond \n",
        "to. This can be done with __gradient ascent in input space__: applying __gradient descent__ to the value of the input image of a convnet so \n",
        "as to maximize the response of a specific filter, starting from a blank input image. The resulting input image would be one that the chosen \n",
        "filter is maximally responsive to.\n",
        "\n",
        "The process is simple: we will build a loss function that maximizes the value of a given filter in a given convolution layer, then we \n",
        "will use stochastic gradient descent to adjust the values of the input image so as to maximize this activation value. For instance, here's \n",
        "a loss for the activation of filter 0 in the layer \"block3_conv1\" of the VGG16 network, pre-trained on ImageNet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXG9sIGgropR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16\n",
        "from keras import backend as K\n",
        "\n",
        "model = VGG16(weights='imagenet',\n",
        "              include_top=False)\n",
        "\n",
        "layer_name = 'block3_conv1'\n",
        "filter_index = 0\n",
        "\n",
        "layer_output = model.get_layer(layer_name).output\n",
        "loss = K.mean(layer_output[:, :, :, filter_index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVKWIYVOropZ",
        "colab_type": "text"
      },
      "source": [
        "To implement gradient descent, we will need the gradient of this loss with respect to the model's input. To do this, we will use the \n",
        "`gradients` function packaged with the `backend` module of Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM1iUvPXropc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The call to `gradients` returns a list of tensors (of size 1 in this case)\n",
        "# hence we only keep the first element -- which is a tensor.\n",
        "grads = K.gradients(loss, model.input)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jLi0BN_ropk",
        "colab_type": "text"
      },
      "source": [
        "A non-obvious trick to use for the gradient descent process to go smoothly is to normalize the gradient tensor, by dividing it by its L2 \n",
        "norm (the square root of the average of the square of the values in the tensor). This ensures that the magnitude of the updates done to the \n",
        "input image is always within a same range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-ANtKYQropm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We add 1e-5 before dividing so as to avoid accidentally dividing by 0.\n",
        "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CXBQGoNrops",
        "colab_type": "text"
      },
      "source": [
        "Now we need a way to compute the value of the loss tensor and the gradient tensor, given an input image. We can define a Keras backend \n",
        "function to do this: `iterate` is a function that takes a Numpy tensor (as a list of tensors of size 1) and returns a list of two Numpy \n",
        "tensors: the loss value and the gradient value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEnIaF9Gropw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "# Let's test it:\n",
        "import numpy as np\n",
        "loss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "B_2q-UWxrop2",
        "colab_type": "text"
      },
      "source": [
        "At this point we can define a Python loop to do stochastic gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtdKFKLkrop5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We start from a gray image with some noise\n",
        "input_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.\n",
        "\n",
        "# Run gradient ascent for 40 steps\n",
        "step = 1.  # this is the magnitude of each gradient update\n",
        "for i in range(40):\n",
        "    # Compute the loss value and gradient value\n",
        "    loss_value, grads_value = iterate([input_img_data])\n",
        "    # Here we adjust the input image in the direction that maximizes the loss\n",
        "    input_img_data += grads_value * step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gctuay_4roqA",
        "colab_type": "text"
      },
      "source": [
        "The resulting image tensor will be a floating point tensor of shape `(1, 150, 150, 3)`, with values that may not be integer within `[0, \n",
        "255]`. Hence we would need to post-process this tensor to turn it into a displayable image. We do it with the following straightforward \n",
        "utility function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnrGqXuvroqD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3S_uc8YroqJ",
        "colab_type": "text"
      },
      "source": [
        "Now we have all the pieces, let's put them together into a Python function that takes as input a layer name and a filter index, and that \n",
        "returns a valid image tensor representing the pattern that maximizes the activation the specified filter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWT5Ydz2roqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_pattern(layer_name, filter_index, size=150):\n",
        "    # Build a loss function that maximizes the activation\n",
        "    # of the nth filter of the layer considered.\n",
        "    layer_output = model.get_layer(layer_name).output\n",
        "    loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "    # Compute the gradient of the input picture wrt this loss\n",
        "    grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "    # Normalization trick: we normalize the gradient\n",
        "    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "    # This function returns the loss and grads given the input picture\n",
        "    iterate = K.function([model.input], [loss, grads])\n",
        "    \n",
        "    # We start from a gray image with some noise\n",
        "    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n",
        "\n",
        "    # Run gradient ascent for 40 steps\n",
        "    step = 1.\n",
        "    for i in range(40):\n",
        "        loss_value, grads_value = iterate([input_img_data])\n",
        "        input_img_data += grads_value * step\n",
        "        \n",
        "    img = input_img_data[0]\n",
        "    return deprocess_image(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipCUN4XroqS",
        "colab_type": "text"
      },
      "source": [
        "Let's try this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTUWkLUDroqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(generate_pattern('block3_conv1', 0))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kTcZjjZroqg",
        "colab_type": "text"
      },
      "source": [
        "It seems that filter 0 in layer `block3_conv1` is responsive to a polka dot pattern.\n",
        "\n",
        "Now the fun part: we can start visualising every single filter in every layer. For simplicity, we will only look at the first 64 filters in \n",
        "each layer, and will only look at the first layer of each convolution block (block1_conv1, block2_conv1, block3_conv1, block4_conv1, \n",
        "block5_conv1). We will arrange the outputs on a 8x8 grid of 64x64 filter patterns, with some black margins between each filter pattern."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4-TQUOtroqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer_name in ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1']:\n",
        "    size = 64\n",
        "    margin = 5\n",
        "\n",
        "    # This a empty (black) image where we will store our results.\n",
        "    results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3))\n",
        "\n",
        "    for i in range(8):  # iterate over the rows of our results grid\n",
        "        for j in range(8):  # iterate over the columns of our results grid\n",
        "            # Generate the pattern for filter `i + (j * 8)` in `layer_name`\n",
        "            filter_img = generate_pattern(layer_name, i + (j * 8), size=size)\n",
        "\n",
        "            # Put the result in the square `(i, j)` of the results grid\n",
        "            horizontal_start = i * size + i * margin\n",
        "            horizontal_end = horizontal_start + size\n",
        "            vertical_start = j * size + j * margin\n",
        "            vertical_end = vertical_start + size\n",
        "            results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img\n",
        "\n",
        "    # Display the results grid\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    plt.imshow(results)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqNjQBNhroqo",
        "colab_type": "text"
      },
      "source": [
        "These filter visualizations tell us a lot about how convnet layers see the world: each layer in a convnet simply learns a collection of \n",
        "filters such that their inputs can be expressed as a combination of the filters. This is similar to how the Fourier transform decomposes \n",
        "signals onto a bank of cosine functions. The filters in these convnet filter banks get increasingly complex and refined as we go higher-up \n",
        "in the model:\n",
        "\n",
        "* The filters from the first layer in the model (`block1_conv1`) encode simple directional edges and colors (or colored edges in some \n",
        "cases).\n",
        "* The filters from `block2_conv1` encode simple textures made from combinations of edges and colors.\n",
        "* The filters in higher-up layers start resembling textures found in natural images: feathers, eyes, leaves, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2JVgWExroqq",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing heatmaps of class activation\n",
        "\n",
        "We will introduce one more visualization technique, one that is useful for understanding which parts of a given image led a convnet to its \n",
        "final classification decision. This is helpful for \"debugging\" the decision process of a convnet, in particular in case of a classification \n",
        "mistake. It also allows you to locate specific objects in an image.\n",
        "\n",
        "This general category of techniques is called \"Class Activation Map\" (CAM) visualization, and consists in producing heatmaps of \"class \n",
        "activation\" over input images. A \"class activation\" heatmap is a 2D grid of scores associated with an specific output class, computed for \n",
        "every location in any input image, indicating how important each location is with respect to the class considered. For instance, given a \n",
        "image fed into one of our \"cat vs. dog\" convnet, Class Activation Map visualization allows us to generate a heatmap for the class \"cat\", \n",
        "indicating how cat-like different parts of the image are, and likewise for the class \"dog\", indicating how dog-like differents parts of the \n",
        "image are.\n",
        "\n",
        "The specific implementation we will use is the one described in [Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via \n",
        "Gradient-based Localization](https://arxiv.org/abs/1610.02391). It is very simple: it consists in taking the output feature map of a \n",
        "convolution layer given an input image, and weighing every channel in that feature map by the gradient of the class with respect to the \n",
        "channel. Intuitively, one way to understand this trick is that we are weighting a spatial map of \"how intensely the input image activates \n",
        "different channels\" by \"how important each channel is with regard to the class\", resulting in a spatial map of \"how intensely the input \n",
        "image activates the class\".\n",
        "\n",
        "We will demonstrate this technique using the pre-trained VGG16 network again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17haNvh8roqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "# Note that we are including the densely-connected classifier on top;\n",
        "# all previous times, we were discarding it.\n",
        "model = VGG16(weights='imagenet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5FJBzberoq1",
        "colab_type": "text"
      },
      "source": [
        "Let's consider the following image of two African elephants, possible a mother and its cub, strolling in the savanna (under a Creative \n",
        "Commons license):\n",
        "\n",
        "![elephants](https://s3.amazonaws.com/book.keras.io/img/ch5/creative_commons_elephant.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X9tjVK5roq4",
        "colab_type": "text"
      },
      "source": [
        "Let's convert this image into something the VGG16 model can read: the model was trained on images of size 224x244, preprocessed according \n",
        "to a few rules that are packaged in the utility function `keras.applications.vgg16.preprocess_input`. So we need to load the image, resize \n",
        "it to 224x224, convert it to a Numpy float32 tensor, and apply these pre-processing rules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3zClb1Yroq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "# The local path to our target image\n",
        "img_path = './creative_commons_elephant.jpg'\n",
        "\n",
        "# `img` is a PIL image of size 224x224\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "# `x` is a float32 Numpy array of shape (224, 224, 3)\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "# We add a dimension to transform our array into a \"batch\"\n",
        "# of size (1, 224, 224, 3)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# Finally we preprocess the batch\n",
        "# (this does channel-wise color normalization)\n",
        "x = preprocess_input(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-FdYjRdrorA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict(x)\n",
        "print('Predicted:', decode_predictions(preds, top=3)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tfQRX6ZrorG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The top-3 classes predicted for this image are:\n",
        "\n",
        "* African elephant (with 92.5% probability)\n",
        "* Tusker (with 7% probability)\n",
        "* Indian elephant (with 0.4% probability)\n",
        "\n",
        "Thus our network has recognized our image as containing an undetermined quantity of African elephants. The entry in the prediction vector \n",
        "that was maximally activated is the one corresponding to the \"African elephant\" class, at index 386:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cWhBcDmrorH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.argmax(preds[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eo_4qbMrorM",
        "colab_type": "text"
      },
      "source": [
        "To visualize which parts of our image were the most \"African elephant\"-like, let's set up the Grad-CAM process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_S5p7AororM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the \"african elephant\" entry in the prediction vector\n",
        "african_elephant_output = model.output[:, 386]\n",
        "\n",
        "# The is the output feature map of the `block5_conv3` layer,\n",
        "# the last convolutional layer in VGG16\n",
        "last_conv_layer = model.get_layer('block5_conv3')\n",
        "\n",
        "# This is the gradient of the \"african elephant\" class with regard to\n",
        "# the output feature map of `block5_conv3`\n",
        "grads = K.gradients(african_elephant_output, last_conv_layer.output)[0]\n",
        "\n",
        "# This is a vector of shape (512,), where each entry\n",
        "# is the mean intensity of the gradient over a specific feature map channel\n",
        "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "# This function allows us to access the values of the quantities we just defined:\n",
        "# `pooled_grads` and the output feature map of `block5_conv3`,\n",
        "# given a sample image\n",
        "iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "\n",
        "# These are the values of these two quantities, as Numpy arrays,\n",
        "# given our sample image of two elephants\n",
        "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
        "\n",
        "# We multiply each channel in the feature map array\n",
        "# by \"how important this channel is\" with regard to the elephant class\n",
        "for i in range(512):\n",
        "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
        "\n",
        "# The channel-wise mean of the resulting feature map\n",
        "# is our heatmap of class activation\n",
        "heatmap = np.mean(conv_layer_output_value, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qb-rszHrorS",
        "colab_type": "text"
      },
      "source": [
        "For visualization purpose, we will also normalize the heatmap between 0 and 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em4D94cCrorT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "heatmap = np.maximum(heatmap, 0)\n",
        "heatmap /= np.max(heatmap)\n",
        "plt.matshow(heatmap)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM_cdoYwrorY",
        "colab_type": "text"
      },
      "source": [
        "Finally, we will use OpenCV to generate an image that superimposes the original image with the heatmap we just obtained:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atjoK-Y4rorZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "# We use cv2 to load the original image\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# We resize the heatmap to have the same size as the original image\n",
        "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "\n",
        "# We convert the heatmap to RGB\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "# We apply the heatmap to the original image\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "# 0.4 here is a heatmap intensity factor\n",
        "superimposed_img = heatmap * 0.4 + img\n",
        "\n",
        "# Save the image to disk\n",
        "cv2.imwrite('/Users/fchollet/Downloads/elephant_cam.jpg', superimposed_img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP4676zIrord",
        "colab_type": "text"
      },
      "source": [
        "![elephant cam](https://s3.amazonaws.com/book.keras.io/img/ch5/elephant_cam.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTT8rcdkrorf",
        "colab_type": "text"
      },
      "source": [
        "This visualisation technique answers two important questions:\n",
        "\n",
        "* Why did the network think this image contained an African elephant?\n",
        "* Where is the African elephant located in the picture?\n",
        "\n",
        "In particular, it is interesting to note that the ears of the elephant cub are strongly activated: this is probably how the network can \n",
        "tell the difference between African and Indian elephants.\n"
      ]
    }
  ]
}